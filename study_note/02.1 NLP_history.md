# Fastcampus NLP

## 자연어처리의 역사

### 규칙기반 및 통계기반 자연어처리

#### 규칙기반 NLP
- 규칙(Rule)에 맞게 처리하는 시스템 → 전문 지식이 필요함  
- 대표적으로 **규칙기반 기계번역**이 있음  
- 데이터를 살펴보면, 전문가가 직접 만들어야 하는 시대  
- **형태소 분석, 구문 분석** 등 여러 분석 방식이 필요  

#### 통계기반 NLP
- 대량의 텍스트 데이터에서 **통계를 활용하여 단어를 표현**  
- "**모두**가 **무의식적으로** 생산한 대량의 데이터를 활용"  
- 특정 단어를 중심으로 주변 단어가 얼마나 등장하는지 카운트  
- 사람이 만든 말뭉치에서 **자동적이고 효율적으로 핵심을 추출**하는 것이 목적  
  - 이를 통해 의미를 **3차원 벡터**로 표현 → **단어의 분산 표현**  
- **SLM(통계적 언어 모델, Statistical Language Model)**  
  - 이전 단어들을 기반으로 다음 단어의 확률을 예측하는 방식 → **확률 기반 접근**  
  - **Sparsity Problem(희소성 문제)**  
    - 충분한 데이터가 없으면 언어를 정확히 모델링하기 어려운 한계 발생  

#### 비교 분석
- **전문가의 중요성 감소**  
- **더 많은 데이터가 필요**  
- **개발이 쉽고 빠름**  
- "**합리주의자 vs 경험주의자**"의 차이와 유사  
  - 현재는 **경험주의자**가 우세한 흐름  

---

### 기계학습 및 딥러닝 기반 자연어처리

"전문가" + "모두" 공존의 시대  

#### 규칙 vs ML & DL
| 규칙기반 모델 | 머신러닝/딥러닝 기반 모델 |
|--------------|----------------------|
| 적은 양의 데이터로 일반화 가능 | 학습 데이터가 많으면 인간의 실력을 넘어설 수 있음 |
| 결론 도출의 논리적인 추론 가능 | 새로운 방법을 발견할 가능성이 있음 |
| 학습에 필요한 데이터가 적음 | 데이터가 많아야 성능이 향상됨 |
| 제작한 전문가의 실력을 넘어서기 어려움 | 결과 해석이 어려움 |
| 오류를 반복하는 경향 | 귀납적 근사로 결론 도출 |
| 규칙 구축에 시간과 비용 소요 |  |
| Toy task에 주로 적용 |  |

- 학습 데이터가 많아질수록 **딥러닝 모델 성능은 지속적으로 상승**하지만, 규칙 기반 모델은 일정 수준에서 성능이 정체됨.

#### 데이터 유형

##### 지도학습 데이터
- 정답을 포함한 데이터가 필요함  
- 수집과 가공에 시간과 금전적 비용이 발생  
- 대량 구축이 어려움  
- 딥러닝 시스템 구축에는 지도학습 과정이 필수적이므로, **데이터 확보 전략**이 중요  

##### 비지도 학습 데이터
- 정답이 필요하지 않은 데이터 (예: **Autoencoder 학습법**)  
- 손쉽게 대량 구축 가능  
- 지도학습을 보완하여 성능 향상 가능  
- 일부 예외를 제외하면, **실제 사용 가능한 수준의 성능 확보 가능**  

---

### Neural Symbolic

#### Symbolic Approaches vs Neural Models

##### Symbolic Approaches
- 기호를 활용해 개념을 정의하고 일정한 논리적 규칙을 통해 추론 가능  
- 일반화 능력이 뛰어나며, 결론 설명이 가능  
- **단점:**  
  - 불완전한 지식 베이스(KB)에 의존  
  - 새롭게 생성되는 지식과의 연결이 어려움  
  - 미분 가능 방식으로 학습이 어려움  
  - Toy Task에 적용이 제한됨  

##### Neural Models
- 대량 데이터를 이용해 다층 신경망으로 귀납적 추론 가능  
- 미분 가능 방식으로 학습 가능하며 높은 정확도 보장  
- **단점:**  
  - 대량 데이터 필요  
  - 학습 도메인에 귀속되어 전이 학습 능력 부족  
  - 결론 도출 과정이 불투명  
  - 외부 지식 활용이 어려움  

##### Neural Symbolic
- Symbolic과 Neural Models의 장점을 결합한 접근법  
- **지식 그래프를 활용하여 딥러닝 모델의 한계를 보완**  
- **KGBERT**: Knowledge Graph의 triples을 텍스트 시퀀스로 변환하여 학습  
- **Common Sense Knowledge Graph**:  
  - 인간의 상식을 엔티티 및 관계로 표현하여 **추론 능력을 극대화**

---

### Pretrain-Finetuning 기반 자연어처리

- **Pre-training**: 대량의 말뭉치로 언어 능력을 학습  
- **Fine-tuning**: 특정 Task에 맞춰 추가 학습  
- **벤치마크 데이터셋** 활용 (훈련 데이터, 검증 데이터, 평가 데이터)  
- **Transformer 계열 모델 발전**  
  - BERT (Encoder)  
  - GPT (Decoder)  
  - BART, T5 (Seq2Seq)  

---

### LLM 기반 자연어처리

- **Upscaling**: 모델 크기가 클수록 성능이 향상됨  
- **Foundation Models**: 다양한 Task를 하나의 모델로 처리 가능  
- **In-Context Few-Shot Learning & Prompt Learning**  
  - 사전학습된 모델이 몇 개의 예제만 보고 새로운 Task를 수행  
  
##### ChatGPT
- **Instruction 기반 학습**  
- **Supervised Fine-tuning (SFT) + Reinforcement Learning with Human Feedback (RLHF)**  

---

### Continual Learning

- 지속적으로 업데이트되는 모델  
- **일반 도메인 모델 → 특정 도메인 모델로 확장**  

---

### Ethics & Fairness

- AI 모델의 **무결성을 보장할 수 없음**  
- 데이터 편향(Bias) 최소화 필요  
- 예: 간호사는 여성이라는 성별 편향  
- **NMT(신경망 기계번역)에서 의미 변질 방지 연구 필요**  
- 오역으로 인한 **경제적 손실, 위법 가능성, 안전 문제** 발생 가능  

## LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS 논문 분석 및 해석

## ABSTRACT
모델이 커질수록 full fine-tuning 방식은 현실적으로 어려워짐. 예를 들어 GPT-3 175B 모델을 독립적으로 미세 조정하여 배포하는 것은 **비용적으로 불가능**함.

### 기존 Fine-Tuning 방식의 문제점
- **매우 많은 파라미터 업데이트 필요**
- **학습 과정에서 GPU 메모리를 많이 사용하며 시간이 너무 오래 걸림**
- **작업별로 새로운 모델이 필요함** → 특정 작업에 맞춰 모델을 fine-tuning할 경우, 작업마다 새로운 가중치를 저장해야 하므로 효율성이 떨어짐.

### LoRA의 핵심 아이디어
해당 논문에서 제시하는 **LoRA (Low-Rank Adaptation)** 방법은 **사전 학습된 모델의 가중치를 고정한 채로**, Transformer 아키텍처의 각 층에 **학습 가능한 저순위(rank decomposition) 행렬을 삽입**하는 방식을 사용하여 다운스트림 작업을 위한 학습해야 할 파라미터 수를 대폭 줄임.

### 논문의 주요 실험 결과
1. **GPT-3 175B 모델을 Adam 옵티마이저로 미세 조정하는 것과 비교했을 때, LoRA는 학습해야 할 파라미터 수를 10,000배 줄이고, GPU 메모리 요구량을 3배 감소시킬 수 있음.**
2. **RoBERTa, DeBERTa, GPT-2, GPT-3 모델에서 전체 미세 조정보다 적은 학습 가능한 파라미터를 사용하면서도 동등하거나 더 나은 성능을 보이며, 더 높은 학습 처리량을 제공함.**
3. **어댑터(adapters)와 달리 LoRA는 추론(inference) 시 추가적인 지연(latency)이 발생하지 않는다는 장점이 있음.**

## INTRODUCTION
### 기존 작업
일부 파라미터만 조정하거나 새로운 task를 위한 외부 모듈을 학습하는 방식으로 모델 배포 문제를 해결하려 함.
- 모델 깊이를 확장하거나 모델의 사용 가능한 시퀀스 길이를 줄여서 추론 지연을 유발함.
- 종종 미세 조정 기준선에 미치지 못해 효율성과 모델 품질 간의 trade-off가 초래됨.
    - trade-off: 둘 중 하나를 포기해야 하는 문제.

### LoRA의 주요 장점
- **모델 재사용**: 사전 훈련된 모델을 공유하고, 작은 LoRA 모듈로 여러 작업을 처리할 수 있음. 이를 통해 저장 용량을 줄이고 작업 전환을 효율적으로 할 수 있음.
- **훈련 효율성**: LoRA는 대부분의 파라미터에 대해 기울기(Gradient)를 계산하지 않으므로 훈련이 더 효율적이며, 하드웨어 요구 사항을 크게 줄일 수 있음.
- **추론 지연 없음**: 훈련 가능한 행렬을 동결된 가중치와 합쳐 배포하면, 완전 미세 조정된 모델과 비교해 추론 지연이 없음.
- **기존 방법과 호환**: LoRA는 기존의 prefix 기반 접근 방식과 자연스럽게 결합될 수 있음. 이 연구에서는 두 가지 LoRA와 prefix-tuning의 조합을 WikiSQL과 MNLI 데이터셋에 대해 평가함.
    - **LoRA+PrefixEmbed (LoRA+PE)**: LoRA와 prefix-embedding 튜닝을 결합한 방식으로, 특수 토큰의 임베딩을 학습 가능한 파라미터로 처리함. 이 방식은 WikiSQL에서 LoRA와 prefix-embedding 튜닝을 모두 능가하는 성능을 보임.
    - **LoRA+PrefixLayer (LoRA+PL)**: LoRA와 prefix-layer 튜닝을 결합한 방식으로, 토큰의 임베딩과 Transformer 블록 후의 활성화를 모두 학습 가능한 파라미터로 처리함. 하지만 학습률에 민감하여 성능이 LoRA보다 다소 떨어짐.
    - LoRA+PE는 WikiSQL에서 뛰어난 성능을 보였고, LoRA+PL은 학습률에 민감해 성능이 조금 더 낮았음.

## Problem statement
사전 훈련된 자기 회귀 언어 모델 PΦ(y|x)를 Φ로 파라미터화하여 주어진다고 가정. 다운스트림 작업은 문맥-대상 쌍의 훈련 데이터셋으로 나타낼 수 있음.
Z = {(xi, yi)}i=1,..,N, 여기서 xi와 yi는 모두 토큰 시퀀스.
- NL2SQL에서는 xi가 자연어 쿼리이고, yi는 그에 해당하는 SQL 명령임. 요약에서는 xi가 기사의 내용이고, yi는 그 요약임.
전체 파인튜닝 중에, 모델은 사전 훈련된 가중치 Φ0으로 초기화되고, 기울기를 반복적으로 따라 Φ0 + ∆Φ로 업데이트되어 조건부 언어 모델링 목표를 최대화함.

각 다운스트림 작업에 대해 서로 다른 파라미터 집합 ∆Φ를 학습한다는 단점이 있음. 이때 ∆Φ의 차원 |∆Φ|는 |Φ0|와 같음.

**LoRA는 ∆Φ = ∆Φ(Θ)를 훨씬 더 작은 크기의 파라미터 집합 Θ로 인코딩함.**
=> ∆Φ를 찾는 작업은 Θ에 대해 최적화하는 문제로 바뀌어짐.

### 개인적인 정리
pre-trained model에서 주어진 파라미터를 fine-tuning하게 되면 파라미터 행렬의 크기만큼 계산을 해야 하는데, GPT-3의 경우 175억이라는 엄청난 크기를 자랑해서 시간이 너무 오래 걸림. 그래서 더 작은 차원의 행렬을 인코딩하여 업데이트하는 것으로 대체함. 차원이 작아진 만큼 속도가 빨라짐.

## 기존 방법 사용하면 안 됨?

### 1. Adapter Layer 추가
어댑터 레이어는 추론 성능에 지연 시간을 추가할 수 있음. 특히 대형 신경망에서 하드웨어 병렬 처리가 없을 경우 성능 저하가 발생함. 어댑터는 적은 파라미터를 사용하지만 순차적으로 처리되어야 하므로, 배치 크기가 작은 온라인 추론 환경에서 성능에 영향을 미침. 모델을 분할할 경우, 추가된 깊이는 더 많은 동기화 GPU 작업을 요구하여 문제를 더욱 악화시킴.

### 2. Input Layer Activation 최적화
프리픽스 튜닝은 최적화가 어려움. 그 성능은 훈련 가능한 파라미터 수에 따라 비정상적으로 변화함. 또한, 시퀀스 길이의 일부를 적응에 할당하면 다운스트림 작업에 필요한 시퀀스 길이를 줄이게 되어, 프롬프트 튜닝은 다른 방법들에 비해 성능이 떨어질 가능성이 있음.

### 결론
두 방법 모두 대규모 및 지연 시간에 민감한 생산 환경에서는 문제가 될 수 있음.


이 밑부터는 완전히 선형대수학을 이용한 이론에 가까운 설명이 가득함
살짝 읽다가 포기하고 자기로 결정.

더 밑은 실험에 대한 결과를 이야기 하고 있음. 해당 논문에서 중요한 점은 이론적인 부분인 것 같으나 이해하기 넘 빡셈. 좀더 시간을 두고 읽어야 할 듯함.


아래는 해석한 논문 링크임
https://arxiv.org/pdf/2106.09685
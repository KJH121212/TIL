# 딥러닝 기반 자연어처리
## 딥러닝
신경망 레이어의 출려값은 레이이ㅓ를 구성하는 가중치들의 값에 의해 결정된다.
m개의 입력을 받아 n개의 값을 출력하는 완전연결층은 m * n개의 입력 가중치 값과 n갸ㅐ의 편향 가중치 값이 있음
딥러닝 모델들에는 입력 데이터와 출력 데이터를 처리하기 위해 보통 수천개이상의 파라미터가 사용되고, 레이어의 수도 수십에서 수백에 이름
이 외에도 모델의 여러 특성들을 결정하는데 가중치 값들이 사용됨. 딥러닝 모델은 수천만에서 수억,수십억 개 이상의 가중치들로 이루어져 있음.
원하는 출력을 만들어내기 위해서는 모든 파라미터의 값을 정밀하게 조정해야함.
딥러닝은 파라미터에 따라 매우 다양한 입력-출력 학습 가능
딥러닝에서의 합습은 수많은 파라미터들의 처적 값을 찾아가는 과정을 의미

- forwardpass: 입력으로부터 예측을 만들어내는 과정
- backwardpass: 예측과 정답사이의 차이를 줄이는 방향으로 파라미터를 수정하는 과정을 의미함

## Seq2Seq
입력된 시퀀스를 다른 시퀀스로 변환하는 모델, 인코더 RNN, 디코더 RNN으로 구성
인코더: 입력 시퀀스를 받아들여 고정된 길이의 벡터로 변환함.(문맥벡터)
디코더: 문맥벡터를 받아들여 출력 시퀀스를 생성

### **순환신경망(RNN, Recurrent Neural Network)**

#### **1. 개념**
RNN은 순차적인 데이터를 처리하는 데 특화된 신경망으로, **이전 단계의 정보를 다음 단계로 전달**할 수 있는 구조를 가진다.  
이를 통해 **시계열 데이터, 자연어 처리(NLP), 음성 인식** 등의 작업에 활용된다.

---

#### **2. 특징**
- **순환 구조 (Recurrent Connection)**  
  → 이전 타임스텝의 정보를 다음 타임스텝에 전달하는 구조  
- **메모리 유지 (Hidden State)**  
  → 과거 정보를 은닉 상태(hidden state)에 저장하여 활용  
- **가중치 공유**  
  → 시퀀스의 길이에 관계없이 동일한 가중치 사용  

---

#### **3. 수식 표현**
##### **(1) 은닉 상태 계산**
\[
h_t = f(W_h h_{t-1} + W_x x_t + b)
\]
- \( h_t \) : 현재 타임스텝의 은닉 상태  
- \( h_{t-1} \) : 이전 타임스텝의 은닉 상태  
- \( x_t \) : 현재 타임스텝의 입력  
- \( W_h, W_x \) : 가중치 행렬  
- \( b \) : 편향  
- \( f \) : 활성화 함수 (주로 tanh)  

##### **(2) 출력 계산**
\[
y_t = g(W_y h_t + b_y)
\]
- \( y_t \) : 현재 타임스텝의 출력  
- \( W_y, b_y \) : 출력층 가중치와 편향  
- \( g \) : 출력층 활성화 함수 (softmax 등)  

---

#### **4. 한계점**
- **장기 의존성 문제 (Long-Term Dependency Problem)**  
  → 시퀀스가 길어질수록 과거 정보가 희석됨  
- **기울기 소실 및 폭발 (Vanishing & Exploding Gradient)**  
  → 역전파 과정에서 기울기가 너무 작아지거나 커져 학습이 어려워짐  

---

#### **5. 개선된 RNN 모델**
##### **(1) LSTM (Long Short-Term Memory)**
- **게이트(Gate) 구조**를 도입하여 **장기 의존성 문제 해결**  
- **입력 게이트, 망각 게이트, 출력 게이트** 활용  

##### **(2) GRU (Gated Recurrent Unit)**
- LSTM보다 **간단한 구조**  
- **업데이트 게이트, 리셋 게이트** 활용  

##### **(3) Bidirectional RNN**
- **양방향 RNN**으로, **과거와 미래의 정보를 모두 활용**  

##### **(4) Transformer 기반 모델**
- **Self-Attention 메커니즘**을 활용하여 RNN 한계를 극복  
- BERT, GPT 등의 최신 NLP 모델에서 사용됨  
 
## Attention
### **Long-term Dependency 문제**
**1. 개념**  
- **긴 시퀀스에서 초반 정보가 후반 예측에 영향을 주기 어려운 문제**  
- 주요 원인: **기울기 소실(Vanishing Gradient)**  

**2. 원인**  
- RNN이 **긴 시퀀스 데이터를 처리할 때 정보가 점점 희석됨**  
- 역전파 과정에서 **기울기가 0에 가까워지면서 학습이 어려워짐**  

### **Attention Mechanism**

**1. 개념**  
- 사람은 글을 읽을 때 **모든 단어에 동일한 집중도를 두지 않음**  
- **중요한 단어에 더 집중하고, 문맥에 따라 주목할 단어를 결정**  

**2. 특징**  
- **Long-term Dependency 문제 해결 가능**  
- **문맥(Context)을 최대한 고려하여 중요한 정보에 가중치 부여**  
- 기존 RNN과 달리 **멀리 떨어진 단어 간의 관계를 효과적으로 학습 가능**  

**3. 주요 원리**  
**✅ 가중치 기반 주목 (Weighted Attention)**
- 입력 단어마다 **중요도를 계산하여 가중치(weight)를 부여**  
- 중요한 단어일수록 **더 높은 가중치**를 가짐  

**✅ Self-Attention (Scaled Dot-Product Attention)**
- 문장에서 **각 단어가 다른 단어와 얼마나 연관이 있는지 계산**  
- 핵심 수식:
  \[
  Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right) V
  \]
  - \( Q \) (Query), \( K \) (Key), \( V \) (Value): 입력 벡터  
  - \( d_k \) : 차원의 크기 (정규화 역할)  

**4. Attention의 활용**  
**✅ Seq2Seq 모델에서의 적용**  
- 기계 번역 (Machine Translation)  
- 문서 요약 (Text Summarization)  

**✅ Transformer 모델에서의 핵심 요소**  
- **BERT, GPT, T5** 등의 최신 NLP 모델에서 사용  
- **RNN 없이 Attention만 사용하여 학습** ("Attention is All You Need" 논문)  

**5. 정리**  
✅ Attention은 **중요한 정보에 가중치를 부여하여 문맥을 효과적으로 학습**  
✅ **Long-term Dependency 문제 해결 가능**  
✅ **Transformer 모델의 핵심 요소로 사용됨**  


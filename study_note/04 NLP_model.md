## 의미기반 언어지식 표현 체계
1. Word2Vec
2. Glove
3. Fasttext
4. Doc2Vec
5. CoVe
## 문맥 기반 모델 종류
1. ELMO
2. GPT-1
3. BERT
4. Encoder 기반 언어모델
5. Decoder 기반 언어모델
## 사전학습 기반 언어모델의 한계점 및 방향성
### 사전학습기반 언어모델의 한계
1. 일반화 능력의 한계
- 라벨링된 데이터의 부족
- Fine tunning 후 모델의 일반화 능력 상실 = pre-training 단계에서 가지고 있던 지식 대부분이 미세조정 과정에서 사라짐.
2. 사람의 선호도 기반 답변 불가
- 사람의 선호도에 대한 정보를 가지고 있지 않기 때문에, 사용자가 원하는 답변을 주기 어려움
3. 지식의 오래됨과 업데이트 문제
- 기존 사전 학습 모델은 사전 학습 당시 사용됐던 데이터의 지식을 바탕으로 학습이 됨. 따라서 변화하는 지식을 대처하기 위해서는 새로 pre-training을 진행해야함.
4. 계산 비용 및 환경 문제
- 사전학습 모델 학습 및 사용은 기술적 혁신을 가져오지만 훈련하고 유지하는데 방대한 자원을 필요로 하기 때문에 탄소배출에 의한 환경 문제를 야기한다.
5. 데이터 편향 및 윤리적 문제
- 특정 코퍼스로 학습되어 해당 코퍼스에서 자주 등장하는 특성으로 편향됨. -> 성별, 인종, 종교 등의 편향을 발생시킴
6. 할루시네이션
- 모델이 사실과 무관하거나 존재하지 않는 정보를 생성하거나, 확인할 수 없는 정보를 생성하는 현상
- Intrinsic Hallucination: Source에 있는 정보들을 사용해서 합성된 잘못된 정보가 결과로 나온 경우
- Extrinsic Hallucination: Source에서 찾을 수 없는 정보가 등장한 경우
### 미래 지향적 언어모델 개발 방향
1. 지속가능한 모델
- Continual Learning : 데이터 수집, 정제, 모델 학습, 테스트까지 자동화하여 지속적인 LM 업데이트
- 새로운 정보를 학습하면서, 이전에 학습한 지식을 잊지 않도록 학습하는 것이 주요 목표
- RAG(Retrieval-Augmented Generation): 생성모델 하나를 사용하는 것이 아닌, 검색 모델과 생성모델을 같이 사용하며 작동 = 데이터베이스 업데이트만으로 정보 업데이트 가능
- Self-Improvement: 인간의 개입 없이 모델 추론 결과를 통해 잘 모르는 지식을 스스로 개선하게 하는 학습 방법 
2. 사람의 선호도 기반 모델
- Human Preference Tuning: 사람 선호도 기반 데이터를 통해 LM을 사람의 선호도가 최상이 되도록 미세조정 하는 방법
3. 계산비용감소
- Hyper parameter 조정
4. 데이터 편향 완화 기술
5. 모델 해석 가능성 및 투명성 강화
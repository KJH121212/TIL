# FASTCAMPUS
## NLP(자연어 처리)

### 개요
#### 1. 개념
컴퓨터가 자연언어의 의미를 분석하고 이해하여 생성할 수 있도록 만들어주는 기술  
- **자연어**: 사람들이 일상생활에서 자연스럽게 사용하는 언어(인공언어의 반대말)

#### 2. NLG vs NLU
- **NLG (Natural Language Generation)**: 자연어 생성, 데이터를 기반으로 의미 있는 문장을 생성하는 기술
- **NLU (Natural Language Understanding)**: 자연어 이해, 입력된 문장을 분석하여 의미를 이해하는 기술

### 자연어 처리가 어려운 이유
1. **중의적 표현**  
   - 언어는 최소한의 표현으로 최대한의 정보를 전달함.  
   - 문장에서 일반적인 정보의 생략이 빈번하여 의미론적인 중의성이 발생함.  
   - 예시:  
     - "길이 있다" → "살아날 길이 있다" / "지나갈 길이 있다"

2. **고유 명사의 처리**  
   - 이름, 회사명 등 특정한 의미를 가지는 단어를 정확히 인식하기 어려움.

3. **사전 미등록어 처리**  
   - 신조어, 줄임말 등이 계속 생성되므로 사전에 등록되지 않은 단어를 처리해야 함.

4. **문맥에 따른 모호성**  
   - 단어나 구문은 주변 문맥에 의해 의미가 변화함.  
   - 예시:  
     - "나는 밥을 먹었다" → 밥이 주식인 경우 '밥'은 '식사'를 의미하지만, 다른 문맥에서는 '밥'이 구체적인 음식을 의미할 수 있음.

### 왜 한국어에서 자연어 처리가 더 어려운가
1. **교착어 특성**  
   - 한국어는 어근과 접사에 의해 단어의 의미와 기능이 정해짐.  
   - 예: '그녀' 뒤에는 '가', '를', '의' 등의 다양한 접사가 붙을 수 있어 조합이 많음.

2. **단어 순서 및 주어 생략**  
   - 단어의 순서가 문장의 의미를 결정하는 요소가 아님.  
   - 주어가 자주 생략됨.

3. **띄어쓰기의 불완전성**  
   - 띄어쓰기가 정착되지 않아 분석이 어려움.  
   - 예: "지금 **막 차**가 떠났다" vs. "지금 **막차**가 떠났다"

4. **평서문과 의문문의 구조가 유사함**  
   - 문장 부호가 없을 경우 구분이 어려움.  
   - 많은 경우 문맥에 의존해야 함.

### 일상 속 자연어 처리 활용 사례
1. **문법 교정 (Grammatical Error Correction, GEC)**  
2. **음성 인식 (Speech Recognition)**  
3. **기계 번역 (Machine Translation)**  
4. **정보 추출 - 검색 (Information Extraction)**  
5. **질의 응답 (Question Answering)**  
6. **문서 요약 (Text Summarization)**  
7. **AI Chat Bot**  
8. **AI 기반 창작 (AI x Creation)**  
9. **자동완성 (Autocomplete)**  

---

## 언어학 기초
### 언어학
- 인간의 언어를 **과학적으로** 연구하는 학문  
- 전산언어학: 컴퓨터를 이용하여 언어를 자동 분석하고 처리하는 연구 분야

### 언어학의 접근 방법
1. **규칙기반 접근**: 문법 규칙을 활용한 방식  
2. **통계기반 접근**: 대량의 언어 데이터를 분석하여 확률적 규칙을 적용  
3. **딥러닝 기반 접근**: 신경망을 활용하여 자동적으로 언어 구조를 학습  

### 음절, 형태소, 어절, 품사
1. **음절**: 가장 작은 말소리 단위  
2. **형태소**: 의미를 가지는 최소 단위  
3. **어절**: 형태소가 모여 띄어쓰기로 구분된 단위  
4. **품사**: 단어를 문법적 의미에 따라 분류한 것  

---

### 언어학의 하위 분야
#### 1. 형태 (Form)
- **음운론 (Phonology)**: 말소리 연구  
- **형태론 (Morphology)**: 형태소와 단어 연구  
- **통사론 (Syntax)**: 문장 구조 연구  

#### 2. 내용 (Meaning)
- **의미론 (Semantics)**: 단어와 문장의 의미 연구  
- **구조적 모호성 (Structural Ambiguity)**  
  - 예: "나는 그녀를 좋아하는 친구와 이야기했다."  
    - ① 내가 좋아하는 친구와 이야기했다.  
    - ② 그녀를 좋아하는 친구와 이야기했다.  
- **의미 자질 (Semantic Features)**  
  - 단어의 의미를 자질들의 조합으로 표현  
  - 예:  
    - "남자" → [+인간, +성인, +남성]  
    - "여자" → [+인간, +성인, -남성]  

#### 3. 사용 (Pragmatics)
- **화용론 (Pragmatics)**: 상황에 따라 달라지는 언어의 의미 연구  
- **문맥(Context)**
  - **물리적 문맥**: 단어가 나타나는 물리적 환경  
  - **언어적 문맥**: 주변 단어와의 관계  
- **직시 표현 (Deixis)**
  - 문맥이 있어야 해석 가능한 표현 (ex: 이, 저, 그)  
- **화행 (Speech Act)**
  - 말하는 행위 자체가 특정한 의미를 가짐 (ex: "우체국이 어디인가요?" → 길을 묻는 행위)  

### 언어학의 적용 사례
1. **키워드 분석**  
   - 텍스트 내에서 중요한 키워드를 추출하여 주제나 핵심 정보를 파악
2. **토큰화 (Tokenization)**  
   - 문장을 단어, 음절, 형태소 등으로 나누어 분석 가능한 단위로 분리
3. **품사 태깅 (POS Tagging)**  
   - 단어를 품사에 따라 분류하여 문장 구조 파악
4. **구문 분석 (Parsing)**  
   - 문장의 구조를 분석하여 구문 트리 형태로 표현
5. **의미/담화 분석**  
   - 문장과 담화의 의미를 분석하여 더 깊은 이해를 도모
6. **개체명 인식 (NER, Named Entity Recognition)**  
   - 텍스트에서 사람, 장소, 기관 등의 고유명사를 식별
7. **문법 교정 (GEC)**  
   - 텍스트에서 문법적인 오류를 찾아 수정하는 기술
8. **의존 구문 분석 (Dependency Parsing)**  
   - 문장의 구성 요소들이 서로 어떻게 의존하는지 분석
9. **BERT**  
   - 다양한 층에서 언어 정보를 반영  
     - 아래층: 표면적 특징  
     - 중간층: 구문적 특징  
     - 상층: 의미론적 특징
10. **LIMIT-BERT (Linguistic Informed Multi-Task BERT)**  
   - 언어적 지식을 결합하여 다양한 작업을 동시에 처리하는 모델
11. **GiBERT**  
   - 특정 언어의 구조적 특성을 반영한 BERT 모델
12. **DMOps (Data Management Operation and Recipes)**  
   - 텍스트 분석에서 데이터 관리와 최적화 작업을 지원하는 프레임워크
13. **텍스트 요약 (Text Summarization)**  
   - 긴 텍스트를 압축하여 핵심적인 정보를 요약하는 기술  
     - 추출적 요약: 중요한 문장이나 구를 그대로 추출  
     - 생성적 요약: 새로운 문장을 생성하여 요약
14. **감정 분석 (Sentiment Analysis)**  
   - 텍스트에서 감정적 상태나 의견을 추출하여 긍정, 부정, 중립으로 분류  
15. **질의 응답 시스템 (Question Answering)**  
   - 주어진 질문에 대해 적절한 답을 텍스트에서 찾아 제공하는 시스템

## 텍스트 전처리
### 개요
#### 전처리(Preprocessing) 
1. HTML 태그, 특수문자, 이모티콘
2. 정규표현식
3. 불용어(Stopword)
4. 어간 추출(Stemming)
5. 표제어추출(Lemmatizing)

**preprocessing pipeline**  
문서 → 토큰화(정규표현식/트위터) → 텍스트 전처리(불용어 제거, 어근추출, 구두점(punctuation)) → 단어주머니

##### Preprocessing Tools
**KoNLPy**
- 한국어 자연언어처리를 위한 대표적인 Python Library
- Twitter, Komoran, Mecab 등 다양한 형태소 분석기들을 제공

**NLTK**
- 영어로 된 텍스트의 자연처리를 위한 대표적인 Python Library
- Classification, Tokenization 등 50개가 넘는 library를 제공하며 쉬운 interface 제공

#### 토큰화(Tokenization)
주어진 데이터를 토큰이라 불리는 단위로 나누는 작업  
토큰이 되는 기준은 다를 수 있음  
- character-based Tokenization  
- word-based Tokenization  
- Subword-based Tokenization

1. 문장 토큰화: 문장 분리  
2. 단어 토큰화: 구두점 분리, 단어 분리

토큰화가 중요한 이유: 단어 의미를 밀집 벡터로 표현하기 위해 단어들을 사전화

**고려사항**
1. 구두점이나 특수 문자를 단순히 제외
2. 줄임말과 단어 내 띄어쓰기
3. 문장 토큰화: 단순 마침표를 기준으로 자를 수 없음

**한국어 토큰화의 어려움**  
영어는 합성어나 줄임말에 대한 예외처리만 한다면 띄어쓰기를 기준으로 토큰화가 잘 작동하지만, 한국어는 조사 등의 존재로 띄어쓰기가 불완전해 형태소 단위의 토큰화가 필요합니다.

##### Tokenization Tools
**KoNLPy**
- morphs: 형태소 추출
- pos: 품사 태깅
- nouns: 명사 추출

**SentencePiece**
- Google이 공개한 Tokenization 도구
- BPE, unigram 방식 등 다양한 subword units를 지원

**Tokenizers**
- Huggingface의 대표적인 자연어처리 라이브러리
- 다양한 언어모델들의 Tokenization과 어휘사전 등을 지원

#### 정제(Cleansing)
코퍼스 내에서 의미가 없는 텍스트나 노이즈를 제거하는 작업
- 주로 불용어, 특수문자 제거, 대소문자 통합, 중복 문구 제거, 다중공백 통일 등

1. 불용어
   - 분석에 의미가 없는 단어로 코퍼스 내에서 자주 등장하나 실질적인 의미가 없는 단어들

**NLTK에서는 불용어 사전이 이미 정의되어 있음.**

#### 정규화(Normalization)
- **Stemming (어간 추출)**: 단어에서 접사를 제거하고 어간을 추출
- **Lemmatization (표제어 추출)**: 품사 정보를 보존하며 기본형으로 변환

#### 편집 방법 (Edit Operations)
Levenshtein Distance: 두 문자열 간의 **최소 변환 횟수**를 계산하는 방법  

1. **삭제(Delete)**  
   - 문자열에서 하나의 문자를 제거하는 연산.  
   - 예: "kitten"에서 "k"를 삭제하면 "itten"이 됨.

2. **삽입(Insert)**  
   - 문자열에 하나의 문자를 삽입하는 연산.  
   - 예: "kitten"에 "s"를 삽입하면 "kittens"이 됨.

3. **대체(Substitution)**  
   - 문자열에서 하나의 문자를 다른 문자로 바꾸는 연산.  
   - 예: "kitten"에서 "k"를 "s"로 대체하면 "sitten"이 됨.

이 세 가지 연산은 서로 결합되어 최단 경로로 문자열을 변환하는 데 사용됨. 편집 거리 알고리즘은 주어진 두 문자열을 비교하면서 최소한의 연산으로 한 문자열을 다른 문자열로 변환할 수 있는 방법을 찾음.

##### 예시

- 문자열 "kitten"과 "sitting"의 편집 거리를 구하는 과정:
  1. "k"를 "s"로 대체 → "sitten"
  2. "e"를 "i"로 대체 → "sittin"
  3. "n"을 삽입 → "sitting"

위의 과정에서는 총 3번의 연산이 필요하므로 편집 거리는 3.

#### 정규표현식
특정한 규칙을 가진 문자열을 표현하는 언어로, 복잡한 문자열 검색과 치환에 사용되며, Python에서는 `re` 라이브러리로 처리 가능  
- 복잡한 패턴을 빠르게 처리할 수 있음

### 전처리 기초 실습
#### nltk library
NLTK(Natural Language Toolkit)는 자연어 처리를 위한 파이썬 라이브러리로, 텍스트 전처리, 품사 태깅, 파싱, 말뭉치(Corpus) 처리 등 다양한 기능을 제공함.

---

##### 1. 텍스트 전처리 및 변환

###### 🔹 토큰화 (Tokenization)
| 함수 | 설명 |
|------|------|
| `nltk.word_tokenize(text)` | 문장을 단어 단위로 토큰화 / space와 구두점을 기준으로 토큰화 |
| `nltk.sent_tokenize(text)` | 문장을 문장 단위로 토큰화 |
| `nltk.regexp_tokenize(text, pattern)` | 정규 표현식을 사용한 토큰화 |
| `nltk.tokenize.WordPunctTokenizer().tokenize(text)` | 단어와 구두점을 별도로 분류하는 토크나이저 |
| `nltk.tokenize.TreebankWordTokenizer().tokenize(text)` | **규칙 1.** 하이픈으로 구성된 단어는 하나로 유지<br>**규칙 2.** 아포스트로피로 '접어'가 함께하는 단어는 분리 |




###### 🔹 정규화 (Normalization)
| 함수 | 설명 |
|------|------|
| `nltk.stem.PorterStemmer().stem(word)` | 포터 스테머(Porter Stemmer)를 사용한 어간 추출 |
| `nltk.stem.LancasterStemmer().stem(word)` | 랭커스터 스테머(Lancaster Stemmer)를 사용한 어간 추출 |
| `nltk.stem.SnowballStemmer(language).stem(word)` | 여러 언어를 지원하는 스노우볼 스테머(Snowball Stemmer) |
| `nltk.WordNetLemmatizer().lemmatize(word, pos)` | 표제어 추출(Lemmatization) |

###### 🔹 불용어 처리 (Stopword Removal)
| 함수 | 설명 |
|------|------|
| `nltk.corpus.stopwords.words('english')` | 불용어 리스트 가져오기 |
| `nltk.tokenize.RegexpTokenizer(pattern).tokenize(text)` | 특정 패턴을 사용하여 불용어 필터링 가능 |

---

##### 2. 품사 태깅 (Part-of-Speech Tagging)
| 함수 | 설명 |
|------|------|
| `nltk.pos_tag(tokens)` | 단어 리스트에 대해 품사 태깅 수행 |
| `nltk.help.upenn_tagset(tag)` | 품사 태그의 의미 설명 |
| `nltk.tag.pos_tag_sents(sentences)` | 여러 문장에 대해 품사 태깅 |

---

##### 3. 구문 분석 (Parsing) 및 문법 처리

###### 🔹 문장 구조 분석 (Parsing)
| 함수 | 설명 |
|------|------|
| `nltk.RegexpParser(grammar).parse(tokens)` | 정규 표현식을 사용한 구문 분석 |
| `nltk.ChartParser(grammar).parse(tokens)` | 차트 파서를 사용한 문법 분석 |
| `nltk.EarleyChartParser(grammar).parse(tokens)` | 얼리(Earley) 파서 사용 |

###### 🔹 문법 및 구조 변환
| 함수 | 설명 |
|------|------|
| `nltk.CFG.fromstring(grammar)` | 컨텍스트 자유 문법(CFG) 정의 |
| `nltk.DependencyGrammar.fromstring(dependency_grammar)` | 의존 문법(Dependency Grammar) 정의 |

---

##### 4. 의미 분석 및 개체명 인식 (NER)
| 함수 | 설명 |
|------|------|
| `nltk.ne_chunk(tagged_tokens)` | 개체명 인식(Named Entity Recognition, NER) 수행 |
| `nltk.chunk.regexp.RegexpParser(grammar).parse(tagged_tokens)` | 정규식 패턴을 사용한 개체명 인식 |

---

##### 5. 말뭉치(Corpus) 및 데이터 로드
| 함수 | 설명 |
|------|------|
| `nltk.corpus.gutenberg.fileids()` | 구텐베르크 코퍼스의 파일 목록 가져오기 |
| `nltk.corpus.brown.words()` | 브라운 코퍼스의 단어 가져오기 |
| `nltk.corpus.stopwords.words('english')` | 불용어 목록 가져오기 |
| `nltk.corpus.wordnet.synsets(word)` | WordNet에서 동의어(synonyms) 목록 가져오기 |

---

##### 6. 통계 및 언어 모델링

###### 🔹 빈도 분석
| 함수 | 설명 |
|------|------|
| `nltk.FreqDist(tokens)` | 단어 빈도 계산 |
| `nltk.FreqDist(tokens).most_common(n)` | 가장 자주 등장하는 단어 n개 반환 |

###### 🔹 n-그램 분석
| 함수 | 설명 |
|------|------|
| `nltk.ngrams(sequence, n)` | n-그램 생성 |
| `nltk.bigrams(sequence)` | 바이그램(bigram) 생성 |
| `nltk.trigrams(sequence)` | 트라이그램(trigram) 생성 |

###### 🔹 문장 생성 (언어 모델링)
| 함수 | 설명 |
|------|------|
| `nltk.Text(tokens).generate()` | 간단한 언어 모델 기반 문장 생성 |

---

##### 7. 감성 분석 및 문장 유사도 계산

###### 🔹 감성 분석
| 함수 | 설명 |
|------|------|
| `nltk.sentiment.vader.SentimentIntensityAnalyzer().polarity_scores(text)` | 감성 분석 (긍정, 부정, 중립 점수) |

###### 🔹 유사도 및 의미적 거리 측정
| 함수 | 설명 |
|------|------|
| `nltk.edit_distance(str1, str2)` | 문자열 간 편집 거리(Edit Distance) 계산 |
| `nltk.jaccard_distance(set1, set2)` | 자카드 거리(Jaccard Distance) 계산 |

---

##### 8. 텍스트 시각화
| 함수 | 설명 |
|------|------|
| `nltk.Text(tokens).dispersion_plot(words)` | 특정 단어의 문서 내 분포를 시각화 |
| `nltk.FreqDist(tokens).plot(n)` | 가장 빈번한 n개의 단어 시각화 |

---
## 자연어처리의 다양한 응용시스템

### 자연어이해 기반 하위분야

#### 1. 형태소 분석기
**형태소 분석:** 문자열을 이루는 형태소, 어근, 접두사, 접미사, 품사 등의 구조를 파악하는 과정  
**품사 태깅:** 형태소 분석 결과에 품사 태그를 할당하는 과정  

- **영어:** 대부분의 형태소가 어절 단위로 구분 가능  
- **한국어:** 어절 단위로 형태소가 나뉘지 않음  

##### 형태소 분석 방법
1. **규칙기반 분석**  
   - 전문가들이 직접 문장을 보고 언어학적 규칙에 따라 형태소를 분석  
2. **통계기반 분석**  
   - 모든 가능한 형태소 후보를 생성 후 품사 태깅을 통해 최적의 결과 도출  
3. **딥러닝 기반 분석**  
   - 인공 신경망을 이용하여 형태소 분석을 학습 후 진행  

##### 주요 모델
- **HMM(Hidden Markov Model)**  
  - 은닉 마르코프 모델을 기반으로 한 형태소 분석  
  - 문맥 정보 활용에 한계가 있어 성능이 낮을 수 있음  
- **CRF(Conditional Random Field)**  
  - 시퀀스 라벨링에 활용, 문맥 정보를 직접 활용하여 성능 향상  
- **Character-Level BiLSTM-CRF**  
  - 띄어쓰기 오류 보정을 위해 음절 단위 입력  
  - BiLSTM 레이어를 거쳐 형태소를 분석  

---

#### 2. 개체명 인식 (NER)
**정의:** 문장에서 사람의 이름, 날짜, 기관명 등의 개체명을 추출하는 작업  

##### 태깅 시스템
1. **BIO 시스템**  
   - `B-` (Begin): 개체명의 시작  
   - `I-` (Inside): 개체명 중간  
   - `O-` (Outside): 개체명이 아닌 경우  
2. **BIESO 시스템**  
   - `B-` (Begin): 개체명의 시작  
   - `I-` (Inside): 개체명 중간  
   - `E-` (End): 개체명의 끝  
   - `S-` (Singleton): 단일 개체명  
   - `O-` (Outside): 개체명이 아닌 경우  

##### 활용 사례
- 의료 분야 개체명 인식 (DrugNER, CHEMDNER)  

---

#### 3. 정보 추출
**정의:** 비구조적인 문장에서 <주어, 관계, 목적어> 구조의 정보를 추출  

##### 구조
1. 문서를 문장 단위로 분할  
2. 토큰화  
3. 품사 태깅  
4. 엔티티 추출  
5. 관계 추출 (서로 가까운 엔티티 간 관계 분석)  

##### 접근 방법
1. **규칙 기반 접근**  
   - 사람이 직접 규칙을 정의하여 관계를 추출  
2. **기계학습 기반 접근**  
   - 학습된 모델을 활용하여 관계 추출  
3. **그래프 기반 접근**  
   - 관계를 그래프로 모델링하여 분석  

---

#### 4. 텍스트 분류
**정의:** 문장을 입력받아 특정 클래스에 분류하거나 군집화하는 작업  

##### 감성 분석
- 문장의 감정을 분석하는 자연어처리의 주요 분야  
- **활용 사례:**  
  - 영화 리뷰 감성 분석  
  - 스팸 메일 필터링  
  - 대화 의도 분류  
  - 상품 카테고리 분류  
  - 혐오 표현 탐지  

**의문점**  
- 비꼬는 말투까지 학습이 가능한가?  

### 자연어생산 기반 하위분야

#### 기계번역

##### 발전 과정
1. **Rule Based**: 단어구조, 통사적, 구조적, 의미론적 구조 등을 모두 분석해야 했음  
2. **Statistical Machine Translation (SMT)**: 병렬 코퍼스에서 공기 기반의 통계정보를 바탕으로 번역 수행  
   - 단어 단위 번역 → 구 단위 번역 → 변수 개념 도입 (Hierarchical Phrase-Based SMT)  
   - Pre-reordering 기반 SMT: 번역 전 어순을 변경  
   - Syntax-Based SMT: 변수 조건 추가하여 불필요한 번역 후보 제거  
3. **Neural Machine Translation (NMT)**: Sequence-to-Sequence 구조로 인코더-디코더 방식 활용  

##### 번역 시스템의 목표
- 문장 x가 주어졌을 때, y의 likelihood를 최대로 만드는 y를 찾는 것  

##### 주요 번역 시스템 예시
- 1995년 알타비스타 사의 '바벨피쉬' 번역 서비스: 품질이 낮았음  
- 마이크로소프트 빙 번역, 구글 번역 등의 온라인 서비스 활성화  
- 오프라인 번역 시스템 개발 및 여행용 번역기 시도  

#### 질의응답

##### 과거 정보 검색 기반 질의응답
- **질문처리**: 질문 유형 및 정답 유형 분류  
- **문서처리**: 정답 포함된 문서 또는 문장 검색  
- **정답처리**: 정답 후보 추출  

##### 현대 질의응답 시스템
1. **Machine Reading Comprehension (MRC)**  
   - **IR (Information Retrieval)**: 책장에서 적절한 책을 찾는 역할  
   - **QA (Question Answering)**: 책을 펼쳐 원하는 정보를 제공하는 역할  
2. **대화형 질의응답**: ChatGPT, Bing Chat 등  
3. **Visual Question Answering (VQA)**  
   - Semantic 및 Visual 이해 필요  
   - 기존 VQA는 Single-turn QA, 최근 Multi-modal 확장 연구 진행  
   - Large Vision-Language Model 활용  
   - New VQA Task: Visual Dialog, VideoQA 등 연구 진행  

#### 대화시스템

##### 목적지향 대화 시스템
1. **Pipeline Methods**  
   - **자연어 이해**: 도메인 확인, 의도 파악, 슬롯 채우기 (BIO 스키마 활용)  
   - **대화 상태 추적 (DST: Dialog State Tracking)**: 현재 발화와 대화 히스토리 기반 상태 추적  
   - **자연어 생성**: 발화 정보로 자연어 문장 생성  
   - **음성합성**: 자연어 문장을 음성으로 변환  
2. **종단 간 학습 (End-to-End Learning)**  
   - 기존 파이프라인 방식의 한계 극복  
   - 도메인 확장성 높음  
   - 많은 대화 데이터 필요  
   - 특정 도메인 템플릿 정의를 통해 적은 데이터로 학습 가능  

##### 일상 대화 시스템
1. **검색 기반 방식 (Retrieval-based Methods)**: 기존 응답 데이터에서 적합한 응답 검색  
2. **생성 기반 방식 (Generative Methods)**: RNN 기반 Seq2Seq 모델 활용  
   - 대화 발화 계층을 두어 학습하는 방식 제안  
3. **검색-생성 혼합방식 (Hybrid Methods)**  

#### 문서요약

##### 요약 방식
1. **Extractive Summarization (추출 요약)**: 원문에서 핵심 문장을 직접 추출  
2. **Abstractive Summarization (추상적 요약)**: 원본 텍스트의 의미를 재구성하여 표현  

##### 고급 요약 기법
1. **Multi-document Summarization**: 복수 문서 요약, 다양한 저자의 관점 통합 필요  
2. **Long-document Summarization**  
   - **Divide and Conquer**: 통계적 방법으로 핵심 문장 추출 후 모델 입력  
   - **Attention Modeling**: Sparse attention 도입하여 연산량 절감 (최대 8배 긴 문장 요약 가능)  
3. **Unsupervised Summarization**  
   - 문서의 중요도 점수를 기반으로 요약  
   - 문서를 그래프로 표현하여 노드(Node)와 엣지(Edge) 가중치를 활용  

---

### 자연언어처리의 특이한 분야 요점 정리

#### Hate Speech Detection (혐오 발언 탐지)

##### 정의
인터넷 상에서 발생하는 혐오 발언 및 공격적 표현을 자동으로 탐지하고 분류하는 기술  

##### Dataset
1. **HateXplain**  
   - 3가지 annotation 포함 (hate/offensive/normal)  
   - **Rationale**: post의 레이블링을 결정하는 post의 일부분을 설명  

##### Counter Speech Generation
- 혐오 및 허위정보가 내재되거나 외재된 대화 또는 문장에 대해 모델이 신뢰성 있는 근거가 포함된 문장을 생성하여 적절하게 대응할 수 있도록 하는 Task.  

###### 대표 방법론
1. **Author-Reviewer Framework**: GPT-2를 활용한 생성 모델 평가  
2. **Generate, Prune, Select**: 데이터를 생성한 후 불필요한 부분을 제거(Prune)하고 적절한 데이터를 선택(Select)  

###### 대표 Dataset
1. **CONAN**  
2. **ProsocialDialog**  

---

#### Deception Detection (기만 탐지)

##### Fake News Detection
- 인터넷 상에서 유포되는 정보 중에서 사실과 다른 정보, 혹은 과장된 정보를 식별하고 분류하는 Task  
- **대표 데이터셋**: LIAR  

##### Fact Checking
- 미디어나 인터넷 상에서 유포되는 정보의 진실성을 확인하는 Task  
- **대표 데이터셋**: FEVER  

---

#### Machine Translation (기계 번역)

##### WMT (Workshop on Machine Translation)  

##### Quality Estimation
- 기계번역 된 문장이 얼마나 잘 번역을 하고 있는지의 품질을 예측하는 Task  
- **대표 데이터셋**: QUAK  

##### Automatic Post Editing
- 기계 번역의 출력물에서 번역 오류, 문법적 오류 등을 자동으로 수정하는 Task  
- **대표 데이터셋**: SubEdits  

##### Word-Level AutoCompletion
- 소스 문장, 번역 컨텍스트 및 사람이 입력한 문자 시퀀스가 주어지면 대상 단어를 예측하는 Task  

##### Chat Translation
- 채팅, 일상대화 분야의 구어체에 대해 기계번역을 수행하는 Task  

---

#### Dialogue (대화 처리)

##### Persona-Grounded Dialogue
- 개별 사용자가 갖는 여러 개인적 특성을 고려해 personalized된 대화를 생성하는 Task  
- **대표 데이터셋**: PersonaChat  

##### Persuasive Dialogue
- 상대방을 설득하기 위한 목적의 대화 Task  
- **대표 데이터셋**: Persuasion for Good (건강 관련)  

##### Dialogue Summarization
- 대화 기록을 기반으로 요약하는 Task  
- **대표 데이터셋**: DialogueSum & SAMSum  

##### Knowledge-Grounded Dialogue
- 외부정보가 필요한 경우, 외부 지식을 활용하여 자연스럽고 전문적인 정보를 제공하는 Task  
- **대표 데이터셋**: Wizard Of Wikipedia & Wizard Of Internet  

---

### 기타

##### Question Generation
- 주어진 지문으로부터 질문을 생성하는 Task  
- **대표 데이터셋**: FairytaleQA  

##### Document-Level Relation Extraction
- 문서 전체에서 개체에 대한 속성과 관계를 예측하는 Task  
- **대표 데이터셋**: DocRED  

##### Instruction Tuning
- 사람이 원하는 방식의 대답을 이끌어내기 위한 instruction을 활용하는 방법  
- **대표 데이터셋**: Super Natural Instructions  

##### LLM Evaluation
- LLM의 유창성, 일관성, 정확성 등 다양한 성능 평가 Task  
- **대표 데이터셋**: AI2 Reasoning Challenge, HellaSwag, MMLU, TruthfulQA  

---

### 한국어 관련 Task
- **케어콜 데이터셋** (독거노인 데이터셋)  
- **BEEP!**, **APEACH 데이터셋** (혐오발언 탐지 데이터셋)  
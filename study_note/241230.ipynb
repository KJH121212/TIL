{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastcampus\n",
    "## 파생변수\n",
    "### 파생변수란?\n",
    "기존 변수의 정보를 통대로 정제 및 생성된 새로운 변수\n",
    "1. 함수 변환, 스케일링, 구간화를 기반으로 기존 변수를 새롭게 정제\n",
    "2. 변수들 간의 상호작용 합성, 집계를 통해 새로운 변수를 도출하는 방식\n",
    "#### 파생변수가 중요한 이유\n",
    "1. 성능적인 관점: 알고리즘 입장에서 변수간 연관관계를 파악하기 수월하기 때문에 과적합을 방지할 수 있는 효과\n",
    "2. 해석적인 관점: 도메인 기반의 파생변수는 인간 친화적인 분석관점을 제공\n",
    "3. 파생변수는 정보의 손실을 최소화하면서 압축효과에 따른 신속한 전처리가 가능\n",
    "### 변수 자체을 변환시키는 방법\n",
    "#### 함수변환\n",
    "1. 로그변환\n",
    "수치형 변수의 로그 변환(x에 log를 씌움)\n",
    "- 비대칭된 임의의 분포를 정규분포에 가깝게 전환\n",
    "- 데이터의 정규성은 모델의 성능을 향상\n",
    "- 데이터의 스케일을 작고 균일하게 만들어 데이터 간의 편차를 줄이는 효과\n",
    "\n",
    "2. 제곱근 변환\n",
    "- 로그변환봐 비슷하지만 정도의 차이가 있음. 제곱근 변환이 좀더 약함.\n",
    "\n",
    "3. Box-Cox 변환\n",
    "- 람다의 값을 조절하면서 맞출 수 있다는 장점이 있음\n",
    "\n",
    "#### 스케일링\n",
    "1. Min-Max 스케일링\n",
    "- 연속형 변수의 수치볌위를 0-1 사이로 전환\n",
    "- 각기 다른 연속형 변수의 수치 범위를 통일할 때 사용\n",
    "2. 표준화\n",
    "- 변수의 수치 범위(스케일)을 평균이 0, 표준편차가 1이 되도록 변환\n",
    "#### 구간화\n",
    "#### 기타 변수\n",
    "1. 분할\n",
    "- 이름과 같은 짧은 단위의 문자를 분할하여 통릴성 있는 데이터를 구성\n",
    "- 그룹화를 통해 잠재적인 정보를 추출\n",
    "- ex) 성과 이름이 합쳐진 변수를 성, 이름 2개로 분할\n",
    "2. 시간 변수\n",
    "- 날짜 및 시간 변수는 시점이라는 풍부한 정보를 담고 있기 때문에 모델에 효과적으로 호라용\n",
    "- 시간요소를 분리 추출하거나, 시간차를 파생변수로 변환\n",
    "### 변수간의 관계를 활용해 새로운 변수를 만드는 방법\n",
    "#### 상호작용변수\n",
    "1. 변수 결합\n",
    "- 서로 다른 의미를 담고 있는 변수를 통해 새로운 의미를 파생하기 때문에, 알고리즘 모델의 데이터 해석이 용이하다.\n",
    "- 도메인 지식과 연관하여 변수 도출의 객관성을 유지하는 것이 중요하다.\n",
    "#### 통계기반 변수\n",
    "1. 통계기반 변수\n",
    "- 데이터 집계후 평균, 장앙, 최대, 최소 등의 통게치를 파생변수로 적용\n",
    "- 해당 관측자가 전체에서 차지하는 상대적인 위치를 쉽게 표현 가능\n",
    "\n",
    "## 파생변수 선택 방법\n",
    "### 개요\n",
    "1. 변수 선택(Feature selection)\n",
    "- 학습에 필요한 변수를 중요도에 따라 선택하는 과정\n",
    "2. 시행 이유\n",
    "- 차원의 저주(Curse of Dimensionaliy) 해소\n",
    "### Filter methods\n",
    "변수간의 통계적 관계를 평가해 변수의 중요도를 결정\n",
    "- 통계적 관계: 변수간의 상관관계, 분산 고려\n",
    "- 상관관계: 변수들 간의 상관계수를 계산해, 상관관계가 높은 변수들을 제거\n",
    "- 분산: 분산이 낮은 변수들을 제거해 변동성이 낮은 변수 제거\n",
    "\n",
    "1. 카이제곱 test 기반 변수 선택\n",
    "- 카이제곱 독립 검정: 두 범주형 변수 간의 유의미한 관계가 있는지 검정하는 방법\n",
    "- Target variable과 독립변수들 간에 카이제곱 통계량을 구한다. 이를 통해 p-value를 계산해, 통계량이 충분히 크면 p-value는 충분히 낮고, 이때 귀무가설을 기각한다.  \n",
    "= target variable과 독립변수가 관계가 있다고 판단 할 수 있다.\n",
    "\n",
    "### Wrapper method\n",
    "실제 모델의 성능을 활용해 변수를 선택하는 방법\n",
    "- 모델을 반복적으로 학습시키고 검증하는 과정에서 최적의 변수조합을 찾는 방법\n",
    "- 순차적 특성 제거: 변수를 하나씩 추가하면서 탐색\n",
    "- 재귀적 특성 제거: 변수를 하나씩 제거하면서 탐색\n",
    "\n",
    "1. 전진선택\n",
    "- 아무런 Feature가 없는 상태에서 시작해 하나하나 유의미한 Feature를 추가해 나감  \n",
    "= 유의미의 기준은 모델의 성능 및 평가지표로, 성능이 상승하면 해당 Feature를 추가한다.\n",
    "2. 후진제거\n",
    "- 전체 Feature들 중에서 가장 무의미한 Feature를 제거해 나감\n",
    "- 모든 feature를 가진 모델에서 시작해 모델에 도움이 되지 않는 feature를 하나씩 줄여나감.  \n",
    "= 무의미의 기준은 해당 feature을 제거시, 모델의 성능 하락폭이 얼마나 되는지이다.\n",
    "\n",
    "### Embedded methods\n",
    "모델의 훈련과정에서 변수의 중요도를 평가해 이를 기반ㅇ로 모델에 대한 변수의 기여도를 결정하는 방법\n",
    "1. feature importance 기반\n",
    "- 트리 node 분할에 대해 각 feature의 기여도로 각 변수의 중요성을 판단하는 방법\n",
    "- 트리 split 기준: gini 계수, Entropy -> 학습과정에서 Feature의 기여도를 Importance에 따라 조절\n",
    "\n",
    "2. Regularization기반\n",
    "- L1, L2 등의 Regularization을 사용해 특정 Feature의 weight을 0, 혹은 0으로 가깝게 만들어 Feature 제거의 효과를 본다.\n",
    "\n",
    "### 추가적인 방법\n",
    "1. correlation 기반 selection\n",
    "상관도가 높은 변수가 많을 때 유용함.  \n",
    "변수집합 중, 상관계수가 높은 하위 집합을 찾아, 가장 원소의 개수가 많은 변수를 대표 변수로 선택  \n",
    "\n",
    "2. feature importance 기반\n",
    "학습된 트리 모델의 중요도를 기반\n",
    "\n",
    "3. Permutation importance 기반\n",
    "검증 데이터셋의 Feature를 하나하나 shuffle하며 성능 변화를 관찰하는 방법\n",
    "- shuffle의 의미 = 해당 feature를 Noise로 만드는 과정\n",
    "만약 해당 feature가 중요한 역할을 하고 있었다면 모델 성능이 크게 하락할 것\n",
    "- 검증 데이터셋 feature의 shuffle로 인해 모델이 학습했던 분포와 크게 달라져 모델의 예측 성능이 하락\n",
    "주의할 점\n",
    "- feature 개수가 많으면 효율적이지 못함\n",
    "- Random permutation에 의존하기 때문에, 실행마다 Feature importance 결과가 상이할 수 있음\n",
    "\n",
    "4. Target permutation\n",
    "Shuffle된 Target 변수 모델을 학습시킨 후, Feature importance와  Actual feature importacne를 비교해 변수를 선택\n",
    "1) Null importance 도출: Target 변수를 여러번 임의로 Shuffle해 모델 학습 후 Null importance의 분포 도출\n",
    "2) Original importance 도출: 원래 데이터셋에서 importance 도출\n",
    "3) 1,2에서 구한 importance를 비교해 실제로 중요한 변수를 선택\n",
    "장점: Feature들끼리의 상호작용을 고려가능, 높은 분산을 가지거나 target 변수와 관련 없는 변수들을 쉽게 도출 가능\n",
    "\n",
    "5. Adeversarial validation의 개념\n",
    "학습 데이터셋과 검증데이터셋이 얼마나 유사한지 판단\n",
    "- 학습 데이터셋의 Target값은 1로, 검증 데이터셋은 0으로 지정후, Binary classification 모델링\n",
    "\n",
    "## 머신러닝 기본 모델\n",
    "### 머신러닝 모델의 목적\n",
    "1. 분류: 예측하고 싶은 종속변수가 범주형일때\n",
    "2. 회귀: 예측하고 싶은 종속변수가 연속형일때\n",
    "\n",
    "### 머신러닝 모델의 분류\n",
    "1. 선형모델: x와 y의 관게를 선형으로 매핑\n",
    "2. 비선형 모델: x와 y의 관게를 비선형으로 매핑(KNN, Tree)\n",
    "\n",
    "### Linear Regresion(선형회귀)\n",
    "데이터를 가장 잘 대변하는 치적의 선을 찾는 과정  \n",
    "독립변수들과 연속형 종속변수 사이의 선형 관게를 학습  \n",
    "\n",
    "1. 최소자승법(Ordinary Least Square; OLS)\n",
    "x와 y의 관계를 가장 잘 나타내는 Best line을 찾는 방법  \n",
    "잔차제곱(SSE)의 합을 최소화 하는 회귀계수 베타를 구하는 방법  \n",
    "- 장점: 학습 및 예측 속도가 빠르고 모델의 해석이 명확(화귀계수 해석 가능)\n",
    "- 단점: x와 y의 관게를 선형으로 가정하기 때문에 현실에서는 잘 적용되지 않을 가능성이 높다.(이상치에 다소 민감하다.)\n",
    "\n",
    "#### 선형회귀의 가정\n",
    "1. 선형성\n",
    "독립변수 x와 종속변수 y 사이에는 선형 관계가 성립한다.(시각화를 통해 확인 가능)\n",
    "2. 잔차 관련 과정\n",
    "- 1) 정규성: 잔차들은 평균이 0인 정규분포를 구성\n",
    "- 2) 등분산성: 잔차들은 분산이 일정\n",
    "- Q-Q(Quantile=Quantile) plot을 통해 확인: 잔차를 오름차순으로 나열했을 때의 분위수와 이론적인 잔차의 분위수 값을 비교해서 정규성을 확인\n",
    "3. 독립성\n",
    "독립변수들 간 상관관계가 존재하지 않아야함.  \n",
    "VIF(Variance Inflation Factor, 분산팽창계수)가 10이 넘으면 다중공선성에 문제가 있다고 여겨짐  \n",
    "\n",
    "#### 사용시 주의점\n",
    "1. 이상치 처리\n",
    "IQR을 통해 이상치 처리\n",
    "\n",
    "#### 모델 평가와 결과 해석\n",
    "R-square(결정계수)rk 1에 가까울 수록 완벽한 선형관계를 가짐\n",
    "- 회귀모형 내에서 독립변수들이 설명할 수 있는 종속변수의 변동성\n",
    "\n",
    "### KNN(K-Nearest Neighbor, K-최근접 이웃)\n",
    "가까운 이웃에 위치한 K개의 데이터를 보고, 데이터가 속할 그룹을 판단하는 과정  \n",
    "거리기반 모델, 사례 기반 학습  \n",
    "거리측정방법에 따른 KNN 모델의 성능 차이가 존재한다.\n",
    "L1은 절대값을 사용하다보니 직교 형태의 경계면이 생성되고, L2(유클리드)의 경우 좀더 flexible 한 경계면이 만들어짐.  \n",
    "분류와 회귀를 위한 KNN 뿐 아니라 유사한 데이터(인접한 이웃)을 구하기 위한 방법론으로도 가능  \n",
    "- 장점: 단순하고 특별한 훈련을 거치지 않아 빠르게 수행 가능하며, 데이터에 대한 특별한 가정이 존재하지 않음\n",
    "- 단점: 적절한 K선택이 필요하며, 데이터가 많아지면 분류가 느려지고 데이터 스케일에 민감하기 때문에 스케일링이 필수적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개인 학습\n",
    "프로젝트의 단지분류를 통계 가비나 변수를 이용해 카테고리별로 나누는건 어떤지?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

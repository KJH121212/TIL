# 딥러닝 기반 자연언어처리 응용분야

## 딥러닝 기반 사전 학습 모델
사전 학습된 언어 모델(Pre-trained Language Model)은 대량의 텍스트 데이터를 기반으로 사전 학습되어 다양한 NLP 태스크에 활용될 수 있다. 대표적인 모델로 BERT, GPT, T5, BART 등이 있으며, 전이 학습을 통해 특정 도메인에 맞춰 미세 조정(Fine-tuning)할 수 있다.

### BERT (Bidirectional Encoder Representations from Transformers)
BERT는 Google에서 개발한 Transformer 기반의 사전 학습 모델로, 양방향 문맥(Bidirectional Context)을 학습할 수 있어 높은 성능을 보인다.  
- **Masked Language Model (MLM)**: 문장의 일부 단어를 [MASK]로 가리고 이를 예측하도록 학습.  
- **Next Sentence Prediction (NSP)**: 두 문장이 연속된 문장인지 판별하도록 학습.  
BERT는 다양한 NLP 태스크(질의 응답, 개체명 인식, 의미역 분석 등)에서 활용되며, 이후 RoBERTa, ALBERT, T5 등의 모델이 발전하였다.

### BART (Bidirectional and Auto-Regressive Transformers)
BART는 Facebook AI에서 개발한 Transformer 기반의 인코더-디코더 모델로, BERT와 GPT의 특징을 결합하였다.  
- **손상된 입력 복원(Denoising Autoencoder)** 방식으로 사전 학습.  
- 문서 요약, 기계 번역, 질의 응답 등 생성 기반 NLP 태스크에서 우수한 성능을 보인다.  

## 딥러닝 기반 형태소 분석 & 품사 태깅
형태소 분석(Morphological Analysis)은 문장을 최소 단위로 나누고, 각 단어의 품사를 태깅하는 작업이다.

### CNN 기반 형태소 분석 & 품사 태깅
CNN은 지역적인 특징을 효과적으로 추출할 수 있어, 형태소 분석 및 품사 태깅에서 단어 내 문자(character)-level 패턴을 학습하는 데 유용하다.  
- **입력 데이터**: 문자 단위 임베딩(Character Embedding) 또는 단어 단위 임베딩(Word Embedding).  
- **CNN 특징 추출**: 여러 크기의 커널을 사용하여 형태적 특징을 추출.  
- **최종 예측**: Fully Connected Layer와 Softmax를 이용하여 품사 태그를 부여.  

### BiLSTM 기반 형태소 분석 & 품사 태깅
BiLSTM은 문맥 정보를 고려하여 보다 정확한 형태소 분석과 품사 태깅이 가능하다.  
- **입력 데이터**: 단어 임베딩과 문자 수준의 특징을 결합하여 사용.  
- **양방향 LSTM**: 앞뒤 문맥 정보를 활용하여 시퀀스 태깅 수행.  
- **CRF 적용**: 출력층에서 조건부 확률을 조정하여 태깅 성능을 향상.

### Character-Level BiLSTM-CRF 모델
Character-Level BiLSTM-CRF는 **문자 단위 임베딩**을 사용하여 텍스트의 **국소적인 특징(character-level features)**과 **문맥 정보(contextual information)**를 모두 학습하는 모델이다. 이 모델은 특히 **형태소 분석** 및 **품사 태깅** 작업에서 뛰어난 성능을 발휘한다.

#### 구성 요소
1. **Character-level Embedding**: 
   - 단어를 **문자 단위**로 분해하여 임베딩을 생성한다. 
   - 각 문자는 고유한 임베딩 벡터로 변환되어 입력으로 사용된다. 
   - 이 방법은 특히 형태가 복잡한 언어(예: 한국어, 일본어)에서 효과적이다.
   
2. **BiLSTM (Bidirectional Long Short-Term Memory)**: 
   - BiLSTM은 **양방향 LSTM**으로, 문장의 앞과 뒤 문맥을 모두 고려하여 더 풍부한 정보 학습을 가능하게 한다. 
   - 이를 통해 문맥에 따른 정확한 품사 태깅 및 의미 분석이 가능해진다.

3. **CRF (Conditional Random Field)**:
   - BiLSTM의 출력을 기반으로 **크로스-시퀀스 의존성**을 고려하여 품사 태깅을 개선한다.
   - CRF는 이전 예측 결과를 조건부 확률로 모델링하여 연속적인 예측을 더 일관되게 만든다.
   
#### 특징 및 장점
- **형태소 분석**: 문자 수준에서부터 분석할 수 있어, 복잡한 형태소를 보다 정확하게 분석할 수 있다.
- **저자원 언어**: 단어의 스펠링이나 형태에 관한 정보를 더 많이 활용할 수 있어, 단어 기반 모델에 비해 저자원 언어에서 뛰어난 성능을 보인다.
- **적응력**: 다양한 언어에 맞춰 유연하게 적용할 수 있으며, 기존의 단어 수준 모델보다 더 높은 정확도를 보인다.

### CNN vs BiLSTM vs Character-Level BiLSTM-CRF 비교
| 비교 항목  | CNN 기반 | BiLSTM 기반 | Character-Level BiLSTM-CRF |
|------------|----------|-------------|----------------------------|
| **특징 학습** | 로컬(국소적) 특징에 강함 | 장거리 의존성(Long-term dependency) 학습 가능 | 문자 단위 특징 + 문맥 정보 결합 |
| **처리 속도** | 병렬 연산이 가능하여 빠름 | 순차적 처리로 인해 속도가 느릴 수 있음 | 순차적 처리, 그러나 CRF 적용으로 예측 정확도 향상 |
| **문맥 이해** | 단어 내부(character-level) 패턴을 잘 학습 | 전체 문맥을 고려한 품사 태깅 가능 | 문자 수준에서부터 문맥을 고려한 높은 정확도 |
| **적용 예시** | 한국어와 같이 형태소가 중요한 언어에서 활용 | 문맥을 고려한 언어 모델 기반 분석에 적합 | 형태소 분석 및 품사 태깅에서 우수한 성능 |
| **언어적 장점** | 주로 단어 수준에서 성능 발휘 | 장거리 의존성 모델링이 강점 | 저자원 언어 및 복잡한 형태의 언어에 유리 |

## 딥러닝 기반 의미역 분석 (Semantic Role Labeling, SRL)
의미역 분석은 문장에서 각 단어가 수행하는 의미적 역할을 파악하는 작업이다. 예를 들어,  
> "John sold a book to Mary."  
에서 "John(행위자)", "a book(대상)", "Mary(수신자)"로 역할을 태깅한다.

## 딥러닝 기반 개체명 인식 (NER: Named Entity Recognition)
개체명 인식은 문장에서 고유 명사를 식별하는 작업이다. 예를 들어,  
> "엘론 머스크는 2024년 스페이스X에서 새로운 로켓을 발사했다."  
에서 "엘론 머스크(Person)", "2024년(Date)", "스페이스X(Organization)"으로 분류된다.

### NER 모델
- **BiLSTM-CRF**: 전통적인 문맥 기반 NER 모델.
- **BERT 기반 NER**: 사전 학습된 BERT 모델을 활용하여 문맥 정보를 깊이 이해하고 개체명을 분류.
- **GPT 기반 NER**: 문맥을 바탕으로 개체명을 생성하는 접근 방식.

### NER 응용
- 검색 엔진 최적화(SEO)
- 금융 및 법률 문서 분석
- 의료 데이터 처리
- 챗봇 및 가상 비서

## NER 시스템
NER 시스템은 텍스트 내에서 **고유 명사(Named Entities)**를 추출하고 이를 **정의된 카테고리**로 분류하는 시스템이다. 일반적으로 **사람, 장소, 날짜, 조직, 숫자** 등의 개체를 추출한다.
- **Pipeline NER**: 전통적인 방법으로, 특징을 수동으로 추출하고 머신 러닝 기법을 적용한다.
- **End-to-End NER**: BERT, BiLSTM-CRF와 같은 신경망 기반 모델을 사용하여 데이터 전처리에서부터 후처리까지 한 번에 학습한다.

## Dense Passage Retrieval (DPR)
Dense Passage Retrieval(DPR)은 **질의 응답 시스템**에서 **정보 검색**을 개선하기 위한 방법이다.  
- **DPR**은 전통적인 **TF-IDF**, **BM25** 방식 대신 **밀집 벡터(dense vector)**를 활용하여 더 정확한 검색 결과를 얻는다.  
- **Dual Encoder** 아키텍처를 사용하여 **질의**와 **문서**를 각각 임베딩하고, 이들의 **유사도**를 계산하여 가장 관련성 높은 문서를 검색한다.

## Document Reader
Document Reader는 **질의 응답 시스템**에서 **정보 검색**을 통해 얻은 문서를 기반으로 **정확한 답변을 생성**하는 역할을 한다.  
- **RAG (Retrieval-Augmented Generation)** 모델은 정보를 검색하고, 이를 기반으로 답변을 **생성**하는 방식이다.
- **BERT 기반**의 **reader 모델**은 검색된 문서에서 답변을 추출하는 데 사용된다.

## Information Retrieval to Reader
정보 검색(Information Retrieval, IR) 시스템은 사용자의 질의를 이해하고, 해당 질의와 관련된 문서를 **검색**하는 시스템이다.  
- **IR**의 결과는 **Reader** 모델에 전달되어 **정확한 답변**을 도출하게 된다.
- **DPR**과 같은 **밀집 벡터 검색** 기술이 IR 시스템에서 사용된다.

## BERTSum
BERTSum은 BERT 모델을 기반으로 한 **문서 요약** 모델이다.  
- **추출적 요약**과 **생성적 요약** 모두를 지원한다.  
- BERT를 **문서 요약** 태스크에 맞게 fine-tuning하여 **주요 정보를 추출**하거나 **간결하게 요약**한다.

## STEP (Self-Training with External Predictions)
STEP은 **자기 지도 학습(self-training)** 기법을 활용한 모델 학습 방법이다.  
- **자기 학습** 방식으로 **라벨이 없는 데이터**에서 **모델의 예측**을 활용하여 학습을 수행한다.
- **외부 모델**을 활용하여 예측을 **전달**하고, 이를 **학습에 반영**하여 성능을 향상시킨다.

## PEGASUS
PEGASUS는 **구글**에서 개발한 **문서 요약**에 특화된 모델이다.  
- **Pre-training Objective**으로 **문서 내 주요 부분을 제거(Masked)한 후 예측하는 방식**을 사용한다.  
- **자동 요약(Task-Specific Pre-training)**을 위해 텍스트 생성 모델을 활용한다.

## Dialogue System
Dialogue System은 사용자의 질의에 응답하고, 자연스럽게 **대화**를 이어가는 시스템이다.  
- **Rule-based** 시스템과 **Learning-based** 시스템이 있다.
- **Generative** 또는 **Retrieval-based** 접근법을 사용한다.
- **GPT-3** 같은 모델을 활용하여 **대화 생성** 능력을 향상시킨다.

## TOD_BERT (Task-Oriented Dialogue BERT)
TOD_BERT는 **목표 지향적 대화 시스템**(Task-Oriented Dialogue System)을 위한 BERT 모델이다.  
- 사용자의 **목표 지향적 질의**에 대해 **적절한 응답**을 생성한다.
- BERT 모델을 fine-tuning하여 **의도(Intent)**와 **슬롯(Slot)** 추출을 통해 대화의 흐름을 유지한다.

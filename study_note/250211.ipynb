{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastcampus\n",
    "## Class와 self\n",
    "Class는 변수와 함수를 묶어서 하나의 새로운 객체(type)로 만드는 것.  \n",
    "class를 정의한다 = 새로운 데이터 타입을 정의한 것. 이를 실제로 사용하려면 인스턴스를 생성해야 한다.  \n",
    "인스턴스 : 붕어빵 틀에 반죽을 넣어서 만들어진 붕어빵\n",
    "- 정의된 클래스를 이용해서 인스턴스 생성: 클래스 이름 뒤에 ()를 넣으면 됨.\n",
    "\n",
    "class에 method 추가하기  \n",
    "1. class 내부 함수인 method를 정의하는 것도 def 키워드를 사용.\n",
    "\n",
    "self?\n",
    "1. 'class 내부에 정의된 함수인 method의 첫번째 인자는 반드시 self 여야 한다' 라는 문장이 있을 정도.\n",
    "2. 첫 번째 인자인 self에 대한 값은 파이썬이 자동으로 넘겨준다.\n",
    "3. 클래스 내에 정의된 self는 클래스 인스턴스임\n",
    "4. 즉 self는 객체의 인스턴스 그 자체를 말한다. 즉, 객체 자기 자신을 참조하는 매개변수인 셈\n",
    "5. self 를 붙이지 않고 선언하면 해당 함수 내에서만 사용가능하지만 self가 붙으면 다른 내부함수에서 호출이 가능해짐.\n",
    "\n",
    "## 성능 올리기 방법\n",
    "1. overfitting 방지하기\n",
    "편향과 분산의 관계  \n",
    "모델의 복잡성이 증가하면 분산은 증가하고 편향은 감소한다. 모델의 복잡성이 감소하면 편향은 증가하고 분산은 감소한다.  \n",
    "variance와 Bias 두가지가 동시에 최소화 되는 방향으로 trade-off를 고려해야한다.\n",
    "\n",
    "2. 지역 최솟값 발생\n",
    "- 전체에서 가장 낮은 곳이 아니라, 한 지역 내에서의 최솟값에 머무르게 되었을때 발생함.\n",
    "\n",
    "## 네트워크 안전화 기법\n",
    "1. drop out\n",
    "모델 학습시 임의의 가중치 노드를 일정 확률로 비활성화 시키는 방법 = overfitting 방지  \n",
    "앙상블 기법 중 하나임. \n",
    "Test 시에는 모든 뉴런을 사용 = 학습시보다 뉴런의 수가 많아져 전체적인 active function의 크기가 증가\n",
    "\n",
    "2. feature scaling\n",
    "서로 다른 입력 데이터의 값을 일정한 범위로 맞추는 방법  \n",
    "feature의 크기에 따라서 weight가 달라지기 때문에 맞춰주는 것이 좋은 경우가 많음.  \n",
    "layer를 통과할 수록 분포가 달라지기 때문. 각 층을 통과해도 데이터가 고르게 분포될 수 있도록 layer 마다 정규화 한다.\n",
    "- batch Nortm (batch 단위의 데이터를 기준으로 평균과 분산화/시계열 데이터에서 활용이 어려움 &batch의 크기가 작은 경우 전체 데이터셋에 대한 정보를 반영하기 어려움.)\n",
    "- Layer Norm (개별 데이터 샘플 내에서 모든 특징들의 평균과 분산을 계산하여, batch의 크기와 무관하게 사용 가능)\n",
    "- Instance Norm (각 데이터 샘플의 각 채널에서 평균과 분산을 계산하여 정규화한다. 통상적으로 Image Style Transfer와 같은 분야에서 각 데이터의 고유한 정보를 유지하기 위해 사용된다.)\n",
    "- Group Norm (채널을 여러 그룹으로 나눈 후, 각 그룹 내에서 평균과 분산을 계산하여 정규화 하는 방법/ Batch size 극복을 위한 방안)  \n",
    "등의 방식이 있음\n",
    "\n",
    "3. 가중치 초기화\n",
    "모델의 층이 깊어질수록 active function 이후 데이터의 분포가 한 쪽으로 쏠림.  \n",
    "전역 최소값을 나아가는 과정에서 가장 중요한 부분. plateau와 같은 지점에서 초기화 되면 시간이 오래걸림  \n",
    "모델의 층이 깊어질수록 loss function으로 부터 gradient 값이 점점 작아지거나 커질 수 있음. \n",
    "\n",
    "- 초기화 방법\n",
    "    1. 모델의 가중치를 단순히 모두 0으로 초기화할 경우, loss function의 gradient 가 업데이트가 안되니 가장 최악\n",
    "    2. 균등하게 초기화 할 경우, 출력 값의 범위가 너무 넓어지는 것을 확인 가능. = 안정적인 학습 기대 x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
